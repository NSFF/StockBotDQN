{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nsff591/failed-stock-bot-dqn-project?scriptVersionId=97815469\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Predicting Buy and Sell opportunities - StockBot","metadata":{"id":"TjtSM4z3N4qH"}},{"cell_type":"markdown","source":"disclaimer: This project is a failed project and does not work. I started this project thinking a simple Deep Q-learning Neural network (DQN) was good enough for training a stock investment bot. Through trial and error, this is not the best solution to it. There are some extra nuances that need to be addressed. I would like to link a solution to this project in the following link:  https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n\nIf you still want to run this code, you will need to get an API key from the Alpha Vantage website: https://www.alphavantage.co/\n\nAlternatively, yfinance library might also work and some tweeking to the data format","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"g8pFvpn1OSf-"}},{"cell_type":"markdown","source":"Using The **Alpha Vantage API** to retreive the most recent Stock data\n<img src=\"https://miro.medium.com/max/1400/1*hniu4saxhfVhhKdE4aMnMg.png\" width=\"650\" align=\"centr\"/>","metadata":{"id":"A7fMoREAOXCF"}},{"cell_type":"code","source":"pip install alpha_vantage","metadata":{"id":"Uc6nNRPKZNpT","outputId":"52e2314b-1814-4d8b-b20b-f11a9014af56"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\n\nimport requests #Used to fetch the data from Alpha Vantage API\n\nfrom alpha_vantage.timeseries import TimeSeries \nfrom alpha_vantage.fundamentaldata import FundamentalData\n\nfrom collections import deque\nimport time\nimport random\nimport os\nimport cv2\nfrom PIL import Image\n\nfrom tqdm import tqdm","metadata":{"id":"MV215ht_OWER"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global Constants","metadata":{"id":"fbrMoBFAZmhW"}},{"cell_type":"code","source":"API_KEY = \"\" # insert API key here from Alpha vantage website\nSTOCK_SYMBOL = \"AAPL\"\nALPHA_VANTAGE_OUTPUT_SIZE = \"full\"\nWINDOW_SIZE = 30\nEPOCH_NUM = 20\n\nREPLAY_MEMORY_SIZE = 200\nMIN_REPLAY_MEMORY_SIZE = 50\nMINIBATCH_SIZE = 16\nMODEL_NAME = \"2x4\"\nGAMMA = 0.97\nMIN_REWARD = -200  # For model save\nUPDATE_TARGET_EVERY = 30\n\n# Environment settings\nEPISODES = 10\n\n# Exploration settings\nepsilon = 1  # not a constant, going to be decayed\nEPSILON_DECAY = 0.99975\nMIN_EPSILON = 0.001\n\n#  Stats settings\nAGGREGATE_STATS_EVERY = 50  # episodes\nSHOW_PREVIEW = False","metadata":{"id":"BjY16tAyZmtt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Importing the Data","metadata":{"id":"rfUglf4OQnLr"}},{"cell_type":"code","source":"ts = TimeSeries(key = API_KEY)\ndata, meta_data = ts.get_daily(STOCK_SYMBOL, outputsize = ALPHA_VANTAGE_OUTPUT_SIZE)\n\n## ALL DATES ###\ndata_date = [date for date in data.keys()]\ndata_date.reverse()\n\n## ALL PRIZES ##\n\ndata_close_price = [float(close_prize['4. close']) for close_prize in [data_values for data_values in data.values()]]\ndata_close_price.reverse()","metadata":{"id":"L65NxM1cQlJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(data_date)\nprint(data_close_price)\ndata_close_price = data_close_price[-1000:]","metadata":{"id":"JTNSbT_dbGEW","outputId":"dc0b4650-cf91-46f4-8fb6-086dc54e3680"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale the data\nscaler = MinMaxScaler()\ndata_close_price_scaled = scaler.fit_transform(np.reshape(data_close_price, newshape =(-1,1))).flatten()\n\nprint(data_close_price_scaled)","metadata":{"id":"WXE4U__ff_Ig","outputId":"e9454cb8-5a99-48c0-e45f-dbbedb7399e8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# putting WINDOW_SIZE amount of datapoints in a list which rolls over throughout the data. This is used as input for the neural network\nX_train = []\nfor i in range(WINDOW_SIZE, len(data_close_price_scaled)):\n    X_train.append(data_close_price_scaled[i-WINDOW_SIZE:i])\n\nnp.shape(X_train)","metadata":{"id":"UaFqA_BfAzKJ","outputId":"08e61d1e-d915-45de-dc6d-726c5502a077"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Bot","metadata":{"id":"BAIFyCEnLbgX"}},{"cell_type":"markdown","source":"We are gonna use Deep Q-Networks","metadata":{"id":"sxawM_VBLkrc"}},{"cell_type":"code","source":"class StockBot:\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    self.position_price = 0 # price when bought\n    self.position_status = 0 # 0 = waiting 1 = bought\n    self.profit = 0\n\n  def buy(self, position):\n    self.position_price = position\n    self.position_status = 1 # bought\n\n  def sell(self, current_price):\n    self.profit = self.position_price - current_price\n    self.position_price = 0\n    self.position_status = 0 # waiting","metadata":{"id":"q9jaUGKNgr7a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Environment","metadata":{"id":"YncHmO62ZXX4"}},{"cell_type":"code","source":"class StockEnvironment:\n  def __init__(self,data):\n    self.data = data\n    self.data_len = len(self.data)\n    self.stock_bot = StockBot()\n    self.reset()\n  \n  def reset(self):\n    self.total_profit = 0\n    self.t = 0\n    self.done = False\n    self.new_observation = self.data[self.t]\n    self.stock_bot.reset()\n\n  def step(self, action):\n    reward = 0\n\n    # 0 = waiting; 1 = buying; 2 = selling\n\n    if action == 1: # buying\n      if self.stock_bot.position_status == 0:\n        self.stock_bot.buy(self.data[self.t][-1])\n    elif action == 2:\n      if self.stock_bot.position_status == 1: # if bought, you can sell, otherwise nothing happens\n        self.stock_bot.sell(self.data[self.t][-1])\n        self.total_profit += self.stock_bot.profit \n        #print(\"Total Profit: \" + str(self.total_profit))\n        reward += self.stock_bot.profit\n\n    # clipping reward\n    if reward > 0:\n        reward = 1\n    elif reward < 0:\n        reward = -1\n    \n    self.t += 1 # increasing t to the next day\n\n    if(self.t >= self.data_len): # if the end of the data is reached\n      self.done = True\n      new_observation = self.data[self.data_len-1]\n    else:\n      new_observation = self.data[self.t] # the next price\n\n    return new_observation, reward , self.done\n\n","metadata":{"id":"Z0vZnhjYlPAm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the agent","metadata":{"id":"BQkHpN6DZVaX"}},{"cell_type":"code","source":"class DQNAgent:\n  def __init__(self):\n    # main model that gets trained every step\n    self.model = self.create_model()\n\n    # Target model that we .predict every step\n    self.target_model = self.create_model()\n    self.target_model.set_weights(self.model.get_weights())\n\n    self.replay_memory = deque(maxlen = REPLAY_MEMORY_SIZE)\n\n    self.target_update_counter = 0\n\n  def create_model(self):\n    model = Sequential()\n    # position, current price as inputs\n    model.add(LSTM(units = 64, input_shape= (WINDOW_SIZE, 1)))\n    model.add(Dense(units = 32, activation = 'relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(units = 16, activation = 'relu'))\n    # buy, sell , wait outputs\n    model.add(Dense(units = 3, activation = 'softmax')) \n\n    model.compile(optimizer= 'adam', loss='mse', metrics=['mse'])\n\n    return model\n\n  def update_replay_memory(self, transition):\n    self.replay_memory.append(transition)\n\n  # Queries main network for Q values given current observation space (environment state)\n  def get_qs(self, state):\n        return self.model.predict(np.array(state).reshape(1,-1))\n\n  def train(self, terminal_state, step):\n    if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n      return\n\n    if len(self.replay_memory) > REPLAY_MEMORY_SIZE:\n      self.replay_memory.pop(0)\n\n    minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n    current_states = np.array([transition[0] for transition in minibatch])\n\n    current_qs_list = self.model.predict(current_states)\n\n    new_current_states = np.array([transition[3] for transition in minibatch])\n    future_qs_list = self.target_model.predict(new_current_states)\n\n    X = []\n    y = []\n\n    for index, (current_state, action, reward, new_current_states, done) in enumerate(minibatch):\n      if not done:\n        max_future_q = np.max(future_qs_list[index])\n        new_q = reward + GAMMA * max_future_q\n      else:\n        new_q = reward\n\n      current_qs = current_qs_list[index]\n      current_qs[action] = new_q\n      X.append(current_state)\n      y.append(current_qs)\n\n    self.model.fit(np.array(X),np.array(y), batch_size = MINIBATCH_SIZE, verbose = 0, shuffle = False)\n\n    #print(\"number of model.fit : \"+ str(step))\n    # updating to determine if we want to update target_model yet\n    self.target_update_counter += 1\n\n    if self.target_update_counter > UPDATE_TARGET_EVERY:\n      self.target_model.set_weights(self.model.get_weights())\n      self.target_update_counter = 0","metadata":{"id":"oaOSrtzULeob"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the agent","metadata":{"id":"NsqDLr9iZQJs"}},{"cell_type":"code","source":"env = StockEnvironment(X_train)\nprint(env.reset())\n# short example of what StockEnvironment returns (the next observation, the reward, the end goal reached?)\n# for _ in range(3):\n#     pact = np.random.randint(3)\n#     print(pact)\n#     print(env.step(pact))","metadata":{"id":"Y1TrnM5eVGdL","outputId":"02450b13-3f96-423a-d473-ccb2f4130765"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ep_rewards = [-200]\n\nagent = DQNAgent()\n\n# Iterate over episodes\nfor episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n\n    # Restarting episode - reset episode reward and step number\n    episode_reward = 0\n    step = 1\n\n    print(step)\n    # Reset environment and get initial state\n    env.reset()\n    current_state = env.new_observation\n    \n    # Reset flag and start iterating until episode ends\n    done = False\n    while not done:\n    #while step < EPOCH_NUM:\n\n        # This part stays mostly the same, the change is to query a model for Q values\n        if np.random.random() > epsilon:\n            # Get action from Q table\n            action = np.argmax(agent.get_qs(current_state))\n        else:\n            # Get random action\n            action = np.random.randint(3)\n\n        new_state, reward, done = env.step(action)\n\n        # Transform new continous state to new discrete state and count reward\n        episode_reward += reward\n\n        # if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n        #     env.render()\n\n        # Every step we update replay memory and train main network\n        agent.update_replay_memory((current_state, action, reward, new_state, done))\n        agent.train(done, step)\n        \n        if step % 25 == 0:\n          print(step)\n\n        current_state = new_state\n        step += 1\n\n        # Decay epsilon\n        if epsilon > MIN_EPSILON:\n            epsilon *= EPSILON_DECAY\n            epsilon = max(MIN_EPSILON, epsilon)\n\n    # Append episode reward to a list and log stats (every given number of episodes)\n    ep_rewards.append(episode_reward)\n    # if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n    #     average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n    #     min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n    #     max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n\n        # Save model, but only when min reward is greater or equal a set value\n        # if min_reward >= MIN_REWARD:\n        #     agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\nagent.model.save(\"./model.h5\")","metadata":{"id":"IP_1f045JCJn","outputId":"a995e456-e24e-4a5f-edb3-e1d8a4569393"},"execution_count":null,"outputs":[]}]}